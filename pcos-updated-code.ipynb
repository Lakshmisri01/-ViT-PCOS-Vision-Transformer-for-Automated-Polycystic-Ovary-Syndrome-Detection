{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T09:11:15.506386Z","iopub.status.busy":"2024-03-21T09:11:15.505389Z","iopub.status.idle":"2024-03-21T09:11:28.889457Z","shell.execute_reply":"2024-03-21T09:11:28.888228Z","shell.execute_reply.started":"2024-03-21T09:11:15.506339Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","from torchvision import datasets, transforms, models\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import wandb\n","from tqdm.notebook import tqdm\n","import torch.nn as nn\n","import timm"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:12:53.354275Z","iopub.status.busy":"2024-03-21T09:12:53.353870Z","iopub.status.idle":"2024-03-21T09:12:54.649120Z","shell.execute_reply":"2024-03-21T09:12:54.647715Z","shell.execute_reply.started":"2024-03-21T09:12:53.354245Z"},"trusted":true},"outputs":[],"source":["\n","# data_dir = \"C:/Semester 6 Final Projects/archive (3)/archive (4)/data\"\n","# transform = transforms.Compose([\n","#     transforms.Resize((224, 224)),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","# dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","\n","# train_size = int(0.7 * len(dataset))\n","# val_size = int(0.15 * len(dataset))\n","# test_size = len(dataset) - train_size - val_size\n","# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n","\n","# class_to_idx = dataset.class_to_idx\n","# idx_to_class = {value: key for key, value in class_to_idx.items()}\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:13:44.787128Z","iopub.status.busy":"2024-03-21T09:13:44.786698Z","iopub.status.idle":"2024-03-21T09:13:45.140812Z","shell.execute_reply":"2024-03-21T09:13:45.139611Z","shell.execute_reply.started":"2024-03-21T09:13:44.787095Z"},"trusted":true},"outputs":[],"source":["# transform = transforms.Compose([\n","#     transforms.Resize((224, 224)),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","\n","# train_data_dir = \"D:/Semester 6 Materials/DLSIP/PCOS_Clean/archive (4)/data/train\"\n","# test_data_dir = \"D:/Semester 6 Materials/DLSIP/PCOS_Clean/archive (4)/data/test\"\n","\n","# train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n","\n","# test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n","# test_size = int(0.7 * len(test_dataset))\n","# val_size = int(len(test_dataset)-test_size)\n","# test_dataset,val_dataset = torch.utils.data.random_split(test_dataset,[test_size,val_size])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:13:55.225433Z","iopub.status.busy":"2024-03-21T09:13:55.224549Z","iopub.status.idle":"2024-03-21T09:13:55.230604Z","shell.execute_reply":"2024-03-21T09:13:55.229585Z","shell.execute_reply.started":"2024-03-21T09:13:55.225396Z"},"trusted":true},"outputs":[],"source":["# class_to_idx = train_dataset.class_to_idx\n","# idx_to_class = {value: key for key, value in class_to_idx.items()}"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:14:02.870983Z","iopub.status.busy":"2024-03-21T09:14:02.870191Z","iopub.status.idle":"2024-03-21T09:14:02.879366Z","shell.execute_reply":"2024-03-21T09:14:02.878176Z","shell.execute_reply.started":"2024-03-21T09:14:02.870945Z"},"trusted":true},"outputs":[],"source":["# class_to_idx"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Dataset:\n","infected: 300 samples\n","notinfected: 300 samples\n","\n","Validation Dataset:\n","infected: 42 samples\n","notinfected: 48 samples\n","\n","Test Dataset:\n","infected: 258 samples\n","notinfected: 252 samples\n"]}],"source":["# import torch\n","# from torchvision import datasets, transforms\n","# from torch.utils.data import Subset, DataLoader\n","\n","# # Define the transformation\n","# transform = transforms.Compose([\n","#     transforms.Resize((224, 224)),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","\n","# train_data_dir = \"C:/Semester 6 Final Projects/archive (3)/archive (4)/data/test\"\n","# test_data_dir = \"C:/Semester 6 Final Projects/archive (3)/archive (4)/data/test\"\n","\n","# # Load the datasets\n","# train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n","# test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n","\n","# # Function to get balanced subset indices\n","# def get_balanced_indices(dataset, num_samples_per_class):\n","#     class_indices = {class_name: [] for class_name in dataset.classes}\n","#     for idx, (img, label) in enumerate(dataset):\n","#         class_name = dataset.classes[label]\n","#         class_indices[class_name].append(idx)\n","    \n","#     balanced_indices = []\n","#     for class_name, indices in class_indices.items():\n","#         balanced_indices.extend(indices[:num_samples_per_class])\n","    \n","#     return balanced_indices\n","\n","# # Get balanced indices for train and test sets\n","# train_balanced_indices = get_balanced_indices(train_dataset, 300)\n","# test_balanced_indices = get_balanced_indices(test_dataset, 300)\n","\n","# # Create Subset objects for balanced datasets\n","# train_subset = Subset(train_dataset, train_balanced_indices)\n","# test_subset = Subset(test_dataset, test_balanced_indices)\n","\n","# # Optionally, create a validation dataset from the test dataset if needed\n","# val_size = int(0.15 * len(test_subset))\n","# test_size = len(test_subset) - val_size\n","# test_subset, val_subset = torch.utils.data.random_split(test_subset, [test_size, val_size])\n","\n","# # Create DataLoaders for balanced datasets\n","# train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n","# test_loader = DataLoader(test_subset, batch_size=32, shuffle=True)\n","# val_loader = DataLoader(val_subset, batch_size=32, shuffle=True)\n","\n","# # Verify the class distribution\n","# train_class_counts = {class_name: 0 for class_name in train_dataset.classes}\n","# for _, labels in train_loader:\n","#     for label in labels:\n","#         class_name = train_dataset.classes[label]\n","#         train_class_counts[class_name] += 1\n","\n","# test_class_counts = {class_name: 0 for class_name in test_dataset.classes}\n","# for _, labels in test_loader:\n","#     for label in labels:\n","#         class_name = test_dataset.classes[label]\n","#         test_class_counts[class_name] += 1\n","\n","# val_class_counts = {class_name: 0 for class_name in test_dataset.classes}\n","# for _, labels in val_loader:\n","#     for label in labels:\n","#         class_name = test_dataset.classes[label]\n","#         val_class_counts[class_name] += 1\n","\n","# print(\"Train class counts:\", train_class_counts)\n","# print(\"Test class counts:\", test_class_counts)\n","# print(\"Validation class counts:\", val_class_counts)\n","\n","# # Now you have train_loader, test_loader, and val_loader with balanced datasets\n","\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import Subset, DataLoader, random_split\n","\n","# Define the transformation\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_data_dir = \"C:/Semester 6 Final Projects/archive (3)/archive (4)/data/test\"\n","test_data_dir = \"C:/Semester 6 Final Projects/archive (3)/archive (4)/data/test\"\n","\n","# Load the datasets\n","train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n","test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n","\n","# Function to get balanced subset indices\n","def get_balanced_indices(dataset, num_samples_per_class):\n","    class_indices = {class_name: [] for class_name in dataset.classes}\n","    for idx, (img, label) in enumerate(dataset):\n","        class_name = dataset.classes[label]\n","        class_indices[class_name].append(idx)\n","    \n","    balanced_indices = []\n","    for class_name, indices in class_indices.items():\n","        balanced_indices.extend(indices[:num_samples_per_class])\n","    \n","    return balanced_indices\n","\n","# Get balanced indices for train and test sets\n","train_balanced_indices = get_balanced_indices(train_dataset, 300)\n","test_balanced_indices = get_balanced_indices(test_dataset, 300)\n","\n","# Create Subset objects for balanced datasets\n","train_subset = Subset(train_dataset, train_balanced_indices)\n","test_subset = Subset(test_dataset, test_balanced_indices)\n","\n","# Optionally, create a validation dataset from the test dataset if needed\n","val_size = int(0.15 * len(test_subset))\n","test_size = len(test_subset) - val_size\n","test_subset, val_subset = random_split(test_subset, [test_size, val_size])\n","\n","# Create DataLoaders for balanced datasets\n","train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_subset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_subset, batch_size=32, shuffle=True)\n","\n","# Function to count samples per class\n","def count_samples_per_class(loader, idx_to_class):\n","    counts = {class_name: 0 for class_name in idx_to_class.values()}\n","    for _, labels in loader:\n","        for label in labels:\n","            class_name = idx_to_class[label.item()]\n","            counts[class_name] += 1\n","    return counts\n","\n","# Define the class_to_idx and idx_to_class dictionaries\n","class_to_idx = train_dataset.class_to_idx\n","idx_to_class = {value: key for key, value in class_to_idx.items()}\n","\n","# Count samples per class in each dataset\n","train_counts = count_samples_per_class(train_loader, idx_to_class)\n","val_counts = count_samples_per_class(val_loader, idx_to_class)\n","test_counts = count_samples_per_class(test_loader, idx_to_class)\n","\n","# Print the counts\n","print(\"Training Dataset:\")\n","for class_name, count in train_counts.items():\n","    print(f\"{class_name}: {count} samples\")\n","\n","print(\"\\nValidation Dataset:\")\n","for class_name, count in val_counts.items():\n","    print(f\"{class_name}: {count} samples\")\n","\n","print(\"\\nTest Dataset:\")\n","for class_name, count in test_counts.items():\n","    print(f\"{class_name}: {count} samples\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:14:10.026422Z","iopub.status.busy":"2024-03-21T09:14:10.026015Z","iopub.status.idle":"2024-03-21T09:14:47.893564Z","shell.execute_reply":"2024-03-21T09:14:47.892524Z","shell.execute_reply.started":"2024-03-21T09:14:10.026391Z"},"trusted":true},"outputs":[],"source":["# def count_samples_per_class(dataset, idx_to_class):\n","#     counts = {}\n","#     for _, label in dataset:\n","#         class_name = idx_to_class[label]\n","#         counts[class_name] = counts.get(class_name, 0) + 1\n","#     return counts\n","\n","\n","# class_to_idx = dataset.class_to_idx\n","# idx_to_class = {value: key for key, value in class_to_idx.items()}\n","\n","# train_counts = count_samples_per_class(train_dataset, idx_to_class)\n","# val_counts = count_samples_per_class(val_dataset, idx_to_class)\n","# test_counts = count_samples_per_class(test_dataset, idx_to_class)\n","\n","# print(\"Training Dataset:\")\n","# for class_name, count in train_counts.items():\n","#     print(f\"{class_name}: {count} samples\")\n","\n","# print(\"\\nValidation Dataset:\")\n","# for class_name, count in val_counts.items():\n","#     print(f\"{class_name}: {count} samples\")\n","\n","# print(\"\\nTest Dataset:\")\n","# for class_name, count in test_counts.items():\n","#     print(f\"{class_name}: {count} samples\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:14:52.296130Z","iopub.status.busy":"2024-03-21T09:14:52.295688Z","iopub.status.idle":"2024-03-21T09:15:31.495776Z","shell.execute_reply":"2024-03-21T09:15:31.494710Z","shell.execute_reply.started":"2024-03-21T09:14:52.296098Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["modifying VGG16...\n","modifying VGG19...\n","modifying InceptionV3...\n","modifying GoogLeNet...\n","modifying ResNet50...\n","modifying ResNet152...\n","modifying DenseNet121...\n","modifying DenseNet169...\n","modifying DenseNet201...\n","modifying MobileNetV2...\n","Adding ViT from timm.....\n"]}],"source":["def modify_last_layer(model):\n","    \n","    last_layer = list(model.children())[-1]\n","    \n","    \n","    if isinstance(last_layer, nn.modules.container.Sequential):\n","        num_ftrs = last_layer[-1].out_features\n","    else:\n","        num_ftrs = last_layer.out_features\n","    \n","   \n","    classification_head = nn.Sequential(\n","        nn.Linear(num_ftrs, 2048),\n","        nn.ReLU(),\n","        nn.Linear(2048, 1024),\n","        nn.ReLU(),\n","        nn.Linear(1024, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 256),\n","        nn.ReLU(),\n","        nn.Linear(256,64),\n","        nn.ReLU(),\n","        nn.Linear(64,4)\n","    )\n","\n","    classification_head = classification_head #create_classification_head(num_ftrs)\n","    \n","   \n","    \n","    \n","    model.classification_head = classification_head\n","\n","    \n","    original_forward = model.forward\n","\n","    def new_forward(x):\n","        x = original_forward(x)\n","        return model.classification_head(x)\n","\n","    model.forward = new_forward\n","    \n","    return model\n","\n","\n","\n","def get_vit_model(model_name):\n"," \n","    # Load the pre-trained Vision Transformer model\n","    model = timm.create_model(model_name, pretrained=True)\n","    num_ftrs = model.head.in_features\n","    # Replace the classification head\n","    model.head = nn.Sequential(\n","        nn.Linear(num_ftrs, 2048),\n","        nn.ReLU(),\n","        nn.Linear(2048, 1024),\n","        nn.ReLU(),\n","        nn.Linear(1024, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 256),\n","        nn.ReLU(),\n","        nn.Linear(256,64),\n","        nn.ReLU(),\n","        nn.Linear(64,2)\n","    )\n","    \n","    return model\n","\n","\n","models_and_names = {\n","    \"VGG16\": models.vgg16(pretrained=True),\n","    \"VGG19\": models.vgg19(pretrained=True),\n","    \"InceptionV3\": models.inception_v3(pretrained=True),\n","    \"GoogLeNet\": models.googlenet(pretrained=True),\n","    \"ResNet50\": models.resnet50(pretrained=True),\n","    \"ResNet152\": models.resnet152(pretrained=True),\n","    \"DenseNet121\": models.densenet121(pretrained=True),\n","    \"DenseNet169\": models.densenet169(pretrained=True),\n","    \"DenseNet201\": models.densenet201(pretrained=True),\n","    \"MobileNetV2\": models.mobilenet_v2(pretrained=True),\n","    \n","}\n","\n","\n","results = {}\n","for model_name, model in models_and_names.items():\n","    print(f\"modifying {model_name}...\")\n","    \n","    \n","    for param in model.parameters():\n","        param.requires_grad = True\n","    \n","    model = modify_last_layer(model)\n","    \n","print(\"Adding ViT from timm.....\")\n","\n","models_and_names[\"ViT-B-16\"] = get_vit_model(\"vit_base_patch16_224\")\n","models_and_names[\"ViT-L-16\"] = get_vit_model(\"vit_large_patch16_224\")\n","#models_and_names[\"ViT-L-32\"] = get_vit_model(\"vit_large_patch32_224\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:16:58.805151Z","iopub.status.busy":"2024-03-21T09:16:58.802794Z","iopub.status.idle":"2024-03-21T09:16:58.815668Z","shell.execute_reply":"2024-03-21T09:16:58.814275Z","shell.execute_reply.started":"2024-03-21T09:16:58.805094Z"},"trusted":true},"outputs":[],"source":["import random\n","import string\n","\n","def generate_random_character():\n","    return random.choice(string.ascii_letters)\n","\n","\n","def generate_random_string(length=1):\n","    return ''.join(random.choice(string.ascii_letters) for _ in range(length))\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:17:39.720145Z","iopub.status.busy":"2024-03-21T09:17:39.719491Z","iopub.status.idle":"2024-03-21T09:17:39.770028Z","shell.execute_reply":"2024-03-21T09:17:39.768675Z","shell.execute_reply.started":"2024-03-21T09:17:39.720111Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def test(model,testloader,model_name):\n","    # Test\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(testloader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Plot confusion matrix\n","    confusion_mtx = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.show()\n","    plt.savefig(f\"figures/confusionmatrix/{model_name}.png\")\n","    \n","    return confusion_mtx\n","\n","\n","def calculate_metrics_from_confusion_matrix(confusion_matrix,model_name):\n","  \n","    num_classes = confusion_matrix.shape[0]\n","    overall_precision = 0\n","    overall_recall = 0\n","    overall_f1 = 0\n","    correct = np.trace(confusion_matrix)  # Sum of diagonal elements\n","    total = np.sum(confusion_matrix)\n","    \n","    for c in range(num_classes):\n","        TP = confusion_matrix[c, c]\n","        FP = np.sum(confusion_matrix[:, c]) - TP\n","        FN = np.sum(confusion_matrix[c, :]) - TP\n","        TN = total - TP - FP - FN\n","        \n","        precision = TP / (TP + FP) if TP + FP != 0 else 0\n","        recall = TP / (TP + FN) if TP + FN != 0 else 0\n","        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n","        \n","        overall_precision += precision\n","        overall_recall += recall\n","        overall_f1 += f1\n","    \n","    # Average the metrics across all classes\n","    overall_precision /= num_classes\n","    overall_recall /= num_classes\n","    overall_f1 /= num_classes\n","    accuracy = correct / total\n","    \n","    # Print the metrics\n","    print(f\"Model: {model_name}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Average Precision: {overall_precision:.4f}\")\n","    print(f\"Average Recall: {overall_recall:.4f}\")\n","    print(f\"Average F1 Score: {overall_f1:.4f}\")\n","    \n","    return {\n","        'accuracy': accuracy,\n","        'average_precision': overall_precision,\n","        'average_recall': overall_recall,\n","        'average_f1': overall_f1\n","    }\n","\n","import glob\n","import os\n","import copy\n","\n","# Function to train and evaluate a given model\n","def train_and_evaluate(model, model_name, train_dataset, val_dataset, test_dataset,batch_size=32, num_epochs=50, lr=0.001,verbose_freq=100,other_logs=None):\n","    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","    \n","    \n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    run_name = f\"{model_name}_{generate_random_string(10)}\"\n","    wandb.init(project='transfer-learning-PDMSAPSP-ALL-2', name = run_name)\n","   \n","    hyperparameters = {\n","        \"learning_rate\": lr,\n","        \"batch_size\": 32,\n","        \"epochs\": num_epochs\n","    }\n","    \n","    wandb.config.update(hyperparameters)\n","    wandb.log({\"model_name\": model_name})\n","    if other_logs is not None:\n","        wandb.log(other_logs)\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    # Training\n","    num_epochs = num_epochs\n","    steps = 0\n","    best_model=0\n","    best_acc=0\n","    \n","    for epoch in tqdm(range(num_epochs)):\n","        model.train()\n","        for inputs, labels in tqdm(trainloader):\n","            steps += 1\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Log metrics and details every 250 steps\n","            if steps % verbose_freq == 0:\n","                wandb.log({\"loss\": loss.item()})\n","                # Checkpoint model\n","                \n","                \n","\n","                # Validation\n","                model.eval()\n","                val_loss = 0.0\n","                correct = 0\n","                total = 0\n","                with torch.no_grad():\n","                    for inputs, labels in tqdm(valloader):\n","                        inputs, labels = inputs.to(device), labels.to(device)\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","                        val_loss += loss.item()\n","                        _, predicted = outputs.max(1)\n","                        total += labels.size(0)\n","                        correct += predicted.eq(labels).sum().item()\n","                val_acc = 100* correct / total\n","                wandb.log({\"val acc\":  100* correct / total})\n","                print(correct,total)\n","                \n","                if best_acc<val_acc:\n","                    if os.path.exists(f'{run_name}_{best_model}.pth'):\n","                        os.remove(f'{run_name}_{best_model}.pth')\n","                    best_acc = val_acc\n","                    best_model = steps\n","                    torch.save(model.state_dict(), f'{run_name}_{best_model}.pth')\n","#                     wandb.save(f'{run_name}_{best_model}.pth')\n","                    wandb.log({\"best val acc\": best_acc})\n","\n","\n","                print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {val_loss / len(valloader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n","                \n","    \n","    #Delete all models except best model\n","    saved_models = glob.glob(\"./*pth\")\n","    best_model_name = f\"{run_name}_{best_model}.pth\"\n","    for model_ in saved_models:\n","        if  best_model_name != model_.split(\"/\")[-1]:\n","            os.remove(model_)\n","            \n","    \n","    #Test\n","    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","    best_model = copy.deepcopy(model)\n","    best_model.load_state_dict(torch.load(best_model_name))\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    best_model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(testloader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = best_model(inputs)\n","            _, predicted = outputs.max(1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Plot confusion matrix\n","    confusion_mtx = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')\n","    plt.xlabel('Predicted')\n","    plt.title(f\"{model_name} Best Model Confusion Matrix\")\n","    plt.ylabel('True')\n","    \n","    plt.show()\n","    plt.savefig(f\"{model_name} CM.png\")\n","    wandb.save(f\"{model_name} CM.png\")\n","    \n","    metrics  = calculate_metrics_from_confusion_matrix(confusion_mtx,\"ResNet50\")\n","    wandb.log(metrics)  \n","    wandb.finish()\n","    \n","    return metrics,best_model\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:17:59.645043Z","iopub.status.busy":"2024-03-21T09:17:59.644370Z","iopub.status.idle":"2024-03-21T09:17:59.652580Z","shell.execute_reply":"2024-03-21T09:17:59.651036Z","shell.execute_reply.started":"2024-03-21T09:17:59.645011Z"},"trusted":true},"outputs":[{"data":{"text/plain":["dict_keys(['VGG16', 'VGG19', 'InceptionV3', 'GoogLeNet', 'ResNet50', 'ResNet152', 'DenseNet121', 'DenseNet169', 'DenseNet201', 'MobileNetV2', 'ViT-B-16', 'ViT-L-16'])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["models_and_names.keys()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[1;32m----> 2\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39mkey)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1265\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[1;34m(self, prompt, stream)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1262\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1263\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[1;32m-> 1265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["from getpass import getpass\n","key = getpass(\"Enter key\")\n","wandb.login(key=key)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T09:18:06.180778Z","iopub.status.busy":"2024-03-21T09:18:06.180296Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'val_dataset' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics,best_model \u001b[38;5;241m=\u001b[39m train_and_evaluate(models_and_names[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-L-16\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-L-16\u001b[39m\u001b[38;5;124m\"\u001b[39m,train_dataset, \u001b[43mval_dataset\u001b[49m,test_dataset,lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,other_logs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetune_all\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n","\u001b[1;31mNameError\u001b[0m: name 'val_dataset' is not defined"]}],"source":["\n","metrics,best_model = train_and_evaluate(models_and_names[\"ViT-L-16\"],\"ViT-L-16\",train_dataset, val_dataset,test_dataset,lr=0.0001, num_epochs=10,other_logs={\"finetune_all\": \"True\"})\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["idx_to_class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_embeddings(model, dataloader):\n","    embeddings = []\n","    labels_list = []\n","\n","    def hook(module, input, output):\n","        embeddings.append(output.cpu().numpy())\n","\n","    handle = model.head[8].register_forward_hook(hook)\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            _ = model(inputs)\n","            labels_list.extend(labels.cpu().numpy())\n","    \n","    handle.remove()\n","\n","    embeddings = np.concatenate(embeddings, axis=0)\n","    return embeddings, labels_list\n","\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = models_and_names[\"ViT L16\"]\n","model_path = \"/kaggle/working/ResNet152_oKCzhfWSKa_6900.pth\"\n","model.load_state_dict(torch.load(model_path))\n","model = model.to(device)\n","model.eval()\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","def visualize_tsne(embeddings, labels):\n","    tsne = TSNE(n_components=2, random_state=42)\n","    reduced = tsne.fit_transform(embeddings)\n","\n","    plt.figure(figsize=(12,10))\n","    for i, label in enumerate(np.unique(labels)):\n","        plt.scatter(reduced[labels == label, 0], reduced[labels == label, 1], label=idx_to_class[label])\n","        \n","    \n","    plt.grid(True)\n","    plt.xlabel(\"t-SNE component 1\", fontsize=14)\n","    plt.ylabel(\"t-SNE component 2\", fontsize=14)\n","    plt.title(\"t-SNE visualization of embeddings from Best ResNET152 Model\", fontsize=16)\n","    plt.legend(loc='upper right', fontsize=12)\n","    plt.tight_layout()\n","    \n","    plt.savefig(\"tsne_visualization_resnet152.png\", format=\"png\")\n","    plt.show()\n","\n","   \n","\n","# Extract embeddings and visualize\n","embeddings, labels = extract_embeddings(model, testloader)\n","visualize_tsne(embeddings, labels)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4546896,"sourceId":7772154,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
